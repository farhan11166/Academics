{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b564de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy:\n",
      "π(S1) = a1\n",
      "π(S2) = a1\n",
      "π(S3) = a1\n",
      "π(S4) = a1\n",
      "\n",
      "Final Value Function:\n",
      "V(S1) = -9.4185\n",
      "V(S2) = -8.6375\n",
      "V(S3) = -6.6642\n",
      "V(S4) = -2.0000\n",
      "V(G) = 0.0000\n",
      "\n",
      "Number of iterations: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Policy Iteration for MDP\n",
    "\n",
    "This module implements Policy Iteration to solve the given MDP.\n",
    "States: S1, S2, S3, S4, G (goal)\n",
    "Actions as specified\n",
    "Transitions with probabilities\n",
    "Cost: 2 per transition (reward = -2)\n",
    "Gamma: 0.9\n",
    "\n",
    "Algorithm:\n",
    "- Initialize policy arbitrarily (all a1)\n",
    "- Repeat until convergence:\n",
    "  - Policy Evaluation: compute V for current policy\n",
    "  - Policy Improvement: update policy to greedy w.r.t. V\n",
    "- Output final policy, V, iterations\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "\n",
    "# MDP definition\n",
    "states = ['S1', 'S2', 'S3', 'S4', 'G']\n",
    "actions = {\n",
    "    'S1': ['a1', 'a2'],\n",
    "    'S2': ['a1', 'a2'],\n",
    "    'S3': ['a1', 'a2'],\n",
    "    'S4': ['a1'],\n",
    "    'G': []\n",
    "}\n",
    "\n",
    "# Transitions: state -> action -> list of (next_state, prob)\n",
    "transitions = {\n",
    "    'S1': {\n",
    "        'a1': [('S2', 0.8), ('S3', 0.2)],\n",
    "        'a2': [('S3', 0.7), ('S4', 0.3)]\n",
    "    },\n",
    "    'S2': {\n",
    "        'a1': [('S1', 0.5), ('S3', 0.4), ('G', 0.1)],\n",
    "        'a2': [('S3', 0.9), ('S4', 0.1)]\n",
    "    },\n",
    "    'S3': {\n",
    "        'a1': [('S2', 0.6), ('G', 0.4)],\n",
    "        'a2': [('S4', 1.0)]\n",
    "    },\n",
    "    'S4': {\n",
    "        'a1': [('G', 1.0)]\n",
    "    },\n",
    "    'G': {}\n",
    "}\n",
    "\n",
    "# Reward: -2 for transitions, 0 at goal\n",
    "reward = -2\n",
    "gamma = 0.9\n",
    "\n",
    "def policy_evaluation(policy, V, theta=1e-6):\n",
    "    \"\"\"Policy Evaluation: iterate until V converges for current policy.\"\"\"\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s == 'G':\n",
    "                continue  # absorbing state, V=0\n",
    "            v = V[s]\n",
    "            # V[s] = sum over next states of prob * (reward + gamma * V[next])\n",
    "            action = policy[s]\n",
    "            expected_value = 0\n",
    "            for next_s, prob in transitions[s][action]:\n",
    "                expected_value += prob * (reward + gamma * V[next_s])\n",
    "            V[s] = expected_value\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "def policy_improvement(policy, V):\n",
    "    \"\"\"Policy Improvement: make policy greedy w.r.t. current V.\"\"\"\n",
    "    policy_stable = True\n",
    "    for s in states:\n",
    "        if s == 'G':\n",
    "            continue\n",
    "        old_action = policy[s]\n",
    "        # Find action that minimizes expected cost (since reward negative, minimize expected value)\n",
    "        min_value = float('inf')\n",
    "        best_action = None\n",
    "        for a in actions[s]:\n",
    "            expected_value = 0\n",
    "            for next_s, prob in transitions[s][a]:\n",
    "                expected_value += prob * (reward + gamma * V[next_s])\n",
    "            if expected_value < min_value:\n",
    "                min_value = expected_value\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "        if old_action != best_action:\n",
    "            policy_stable = False\n",
    "    return policy_stable\n",
    "\n",
    "def policy_iteration():\n",
    "    \"\"\"Main Policy Iteration algorithm.\"\"\"\n",
    "    # Initialize policy: all a1\n",
    "    policy = {s: 'a1' for s in states if s != 'G'}\n",
    "    policy['G'] = None  # no action\n",
    "\n",
    "    # Initialize V: 0 for all\n",
    "    V = {s: 0 for s in states}\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        policy_evaluation(policy, V)\n",
    "        stable = policy_improvement(policy, V)\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    return policy, V, iterations\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_policy, final_V, iters = policy_iteration()\n",
    "    print(\"Final Policy:\")\n",
    "    for s in ['S1', 'S2', 'S3', 'S4']:\n",
    "        print(f\"π({s}) = {final_policy[s]}\")\n",
    "    print(\"\\nFinal Value Function:\")\n",
    "    for s in ['S1', 'S2', 'S3', 'S4', 'G']:\n",
    "        print(f\"V({s}) = {final_V[s]:.4f}\")\n",
    "    print(f\"\\nNumber of iterations: {iters}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
